{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CKdouGpTiWd5",
        "outputId": "ada2f786-6e49-4e14-ee5b-503ea35bb9b8"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UUI4tXF4iAhF",
        "outputId": "0c12cd18-daf1-48ce-ab9e-80b37f368201"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting flask-ngrok\n",
            "  Downloading flask_ngrok-0.0.25-py3-none-any.whl.metadata (1.8 kB)\n",
            "Requirement already satisfied: Flask>=0.8 in /usr/local/lib/python3.11/dist-packages (from flask-ngrok) (3.1.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from flask-ngrok) (2.32.3)\n",
            "Requirement already satisfied: blinker>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from Flask>=0.8->flask-ngrok) (1.9.0)\n",
            "Requirement already satisfied: click>=8.1.3 in /usr/local/lib/python3.11/dist-packages (from Flask>=0.8->flask-ngrok) (8.2.1)\n",
            "Requirement already satisfied: itsdangerous>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from Flask>=0.8->flask-ngrok) (2.2.0)\n",
            "Requirement already satisfied: jinja2>=3.1.2 in /usr/local/lib/python3.11/dist-packages (from Flask>=0.8->flask-ngrok) (3.1.6)\n",
            "Requirement already satisfied: markupsafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from Flask>=0.8->flask-ngrok) (3.0.2)\n",
            "Requirement already satisfied: werkzeug>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from Flask>=0.8->flask-ngrok) (3.1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->flask-ngrok) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->flask-ngrok) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->flask-ngrok) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->flask-ngrok) (2025.7.14)\n",
            "Downloading flask_ngrok-0.0.25-py3-none-any.whl (3.1 kB)\n",
            "Installing collected packages: flask-ngrok\n",
            "Successfully installed flask-ngrok-0.0.25\n",
            "Requirement already satisfied: flask-ngrok in /usr/local/lib/python3.11/dist-packages (0.0.25)\n",
            "Collecting pyngrok\n",
            "  Downloading pyngrok-7.2.12-py3-none-any.whl.metadata (9.4 kB)\n",
            "Requirement already satisfied: Flask>=0.8 in /usr/local/lib/python3.11/dist-packages (from flask-ngrok) (3.1.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from flask-ngrok) (2.32.3)\n",
            "Requirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.11/dist-packages (from pyngrok) (6.0.2)\n",
            "Requirement already satisfied: blinker>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from Flask>=0.8->flask-ngrok) (1.9.0)\n",
            "Requirement already satisfied: click>=8.1.3 in /usr/local/lib/python3.11/dist-packages (from Flask>=0.8->flask-ngrok) (8.2.1)\n",
            "Requirement already satisfied: itsdangerous>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from Flask>=0.8->flask-ngrok) (2.2.0)\n",
            "Requirement already satisfied: jinja2>=3.1.2 in /usr/local/lib/python3.11/dist-packages (from Flask>=0.8->flask-ngrok) (3.1.6)\n",
            "Requirement already satisfied: markupsafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from Flask>=0.8->flask-ngrok) (3.0.2)\n",
            "Requirement already satisfied: werkzeug>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from Flask>=0.8->flask-ngrok) (3.1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->flask-ngrok) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->flask-ngrok) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->flask-ngrok) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->flask-ngrok) (2025.7.14)\n",
            "Downloading pyngrok-7.2.12-py3-none-any.whl (26 kB)\n",
            "Installing collected packages: pyngrok\n",
            "Successfully installed pyngrok-7.2.12\n"
          ]
        }
      ],
      "source": [
        "!pip install flask-ngrok\n",
        "!pip install flask-ngrok pyngrok"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install sumy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GZqg_0s4giw9",
        "outputId": "29e7b9d6-e50a-484f-80c5-e269883ebb47"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting sumy\n",
            "  Downloading sumy-0.11.0-py2.py3-none-any.whl.metadata (7.5 kB)\n",
            "Collecting docopt<0.7,>=0.6.1 (from sumy)\n",
            "  Downloading docopt-0.6.2.tar.gz (25 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting breadability>=0.1.20 (from sumy)\n",
            "  Downloading breadability-0.1.20.tar.gz (32 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: requests>=2.7.0 in /usr/local/lib/python3.11/dist-packages (from sumy) (2.32.3)\n",
            "Collecting pycountry>=18.2.23 (from sumy)\n",
            "  Downloading pycountry-24.6.1-py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: nltk>=3.0.2 in /usr/local/lib/python3.11/dist-packages (from sumy) (3.9.1)\n",
            "Requirement already satisfied: chardet in /usr/local/lib/python3.11/dist-packages (from breadability>=0.1.20->sumy) (5.2.0)\n",
            "Requirement already satisfied: lxml>=2.0 in /usr/local/lib/python3.11/dist-packages (from breadability>=0.1.20->sumy) (5.4.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk>=3.0.2->sumy) (8.2.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk>=3.0.2->sumy) (1.5.1)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk>=3.0.2->sumy) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk>=3.0.2->sumy) (4.67.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.7.0->sumy) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.7.0->sumy) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.7.0->sumy) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.7.0->sumy) (2025.7.14)\n",
            "Downloading sumy-0.11.0-py2.py3-none-any.whl (97 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m97.3/97.3 kB\u001b[0m \u001b[31m902.2 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pycountry-24.6.1-py3-none-any.whl (6.3 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: breadability, docopt\n",
            "  Building wheel for breadability (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for breadability: filename=breadability-0.1.20-py2.py3-none-any.whl size=21693 sha256=e105e0038ac7fb1230c4d7010609c7e9ee685fed2528be94f427578fed008769\n",
            "  Stored in directory: /root/.cache/pip/wheels/4d/57/58/7e3d7fedf51fe248b7fcee3df6945ae28638e22cddf01eb92b\n",
            "  Building wheel for docopt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for docopt: filename=docopt-0.6.2-py2.py3-none-any.whl size=13706 sha256=7840c5d621cc9f6ded587d5d55e6e171d84e99908350c577bd8419bfbb2389fa\n",
            "  Stored in directory: /root/.cache/pip/wheels/1a/b0/8c/4b75c4116c31f83c8f9f047231251e13cc74481cca4a78a9ce\n",
            "Successfully built breadability docopt\n",
            "Installing collected packages: docopt, pycountry, breadability, sumy\n",
            "Successfully installed breadability-0.1.20 docopt-0.6.2 pycountry-24.6.1 sumy-0.11.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6RKEwYLdhxgd",
        "outputId": "b5984e34-1fe2-4689-f546-c03e4e32579d"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sqlite3\n",
        "import pandas as pd\n",
        "\n",
        "# Load the full DataFrame\n",
        "df = pd.read_excel('/content/drive/My Drive/Colab/AS4/STEP4-Champion_Clustering&Classification_Save&Evaluation/Raw_with_predicted_classification_label.xlsx')\n",
        "\n",
        "# Rename columns for consistency\n",
        "df.rename(columns={\n",
        "    'Author': 'author',\n",
        "    'Article Title': 'article_title',\n",
        "    'Document Type': 'document_type',\n",
        "    'Keywords': 'keywords',\n",
        "    'Abstract': 'abstract',\n",
        "    'Times Cited, All Databases': 'times_cited',\n",
        "    'Publication Year': 'publication_year',\n",
        "    'DOI Link': 'doi_link',\n",
        "    'predicted_Classification_label': 'predicted_classification_label'\n",
        "}, inplace=True)\n",
        "\n",
        "# Clean times_cited if it's a string with commas\n",
        "df['times_cited'] = df['times_cited'].replace(',', '', regex=True)\n",
        "df['times_cited'] = pd.to_numeric(df['times_cited'], errors='coerce')\n",
        "df['times_cited'] = df['times_cited'].fillna(0).astype(int)\n",
        "\n",
        "# Create SQLite database and table\n",
        "conn = sqlite3.connect('papers.db')\n",
        "c = conn.cursor()\n",
        "\n",
        "# Updated schema with 'keywords'\n",
        "c.execute('''\n",
        "CREATE TABLE IF NOT EXISTS papers (\n",
        "    id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
        "    article_title TEXT UNIQUE,\n",
        "    abstract TEXT,\n",
        "    author TEXT,\n",
        "    publication_year INTEGER,\n",
        "    document_type TEXT,\n",
        "    keywords TEXT,\n",
        "    times_cited INTEGER,\n",
        "    doi_link TEXT,\n",
        "    predicted_classification_label TEXT\n",
        ")\n",
        "''')\n",
        "\n",
        "conn.commit()\n",
        "\n",
        "# Insert data using pandas to_sql\n",
        "df.to_sql('papers', conn, if_exists='replace', index=False)\n",
        "\n",
        "conn.close()\n",
        "print(\" Database created and populated with all fields.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A55obs0cuPLR",
        "outputId": "4234cbe0-2795-4705-c4ff-d970626ba22b"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Database created and populated with all fields.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Add  real token here (already copied from ngrok dashboard)\n",
        "!ngrok config add-authtoken 303S6vb9iu9Bj1arMHaqauC0BLJ_7AgwQ7KRfbfAoBwBPgndX\n",
        "\n",
        "from flask import Flask, request, jsonify\n",
        "from pyngrok import ngrok\n",
        "import joblib\n",
        "import numpy as np\n",
        "import os\n",
        "import pandas as pd\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from scipy.sparse import load_npz\n",
        "\n",
        "# Sumy imports for summarization\n",
        "from sumy.parsers.plaintext import PlaintextParser\n",
        "from sumy.nlp.tokenizers import Tokenizer\n",
        "from sumy.summarizers.lsa import LsaSummarizer\n",
        "\n",
        "# ==== Load classification model and vectorizers ====\n",
        "MODEL_PATH = '/content/drive/My Drive/Colab/AS4/STEP4-Champion_Clustering&Classification_Save&Evaluation/naive_bayes_bow_ngram.pkl'\n",
        "BOW_VECTORIZER_PATH = '/content/drive/My Drive/Colab/AS4/STEP4-Champion_Clustering&Classification_Save&Evaluation/bow_vectorizer.pkl'\n",
        "NGRAM_VECTORIZER_PATH = '/content/drive/My Drive/Colab/AS4/STEP4-Champion_Clustering&Classification_Save&Evaluation/ngram2_vectorizer.pkl'\n",
        "\n",
        "classif_model = joblib.load(MODEL_PATH)\n",
        "bow_vectorizer = joblib.load(BOW_VECTORIZER_PATH)\n",
        "ngram_vectorizer = joblib.load(NGRAM_VECTORIZER_PATH)\n",
        "\n",
        "label_map = {\n",
        "    0: 'Gene Expression Analysis',\n",
        "    1: 'Sequence Classification',\n",
        "    2: 'Protein Structure Prediction',\n",
        "    3: 'Biological Image Analysis',\n",
        "    4: 'Disease Outcome Prediction'\n",
        "}\n",
        "\n",
        "def predict_paper_category(text):\n",
        "    bow_features = bow_vectorizer.transform([text])\n",
        "    ngram_features = ngram_vectorizer.transform([text])\n",
        "    X = np.hstack([bow_features.toarray(), ngram_features.toarray()])\n",
        "    label_num = classif_model.predict(X)[0]\n",
        "    return label_map[label_num]\n",
        "\n",
        "# ==== Load clustering model, PCA, vectorizer ====\n",
        "output_dir = '/content/drive/My Drive/Colab/AS4/STEP4-Champion_Clustering&Classification_Save&Evaluation'\n",
        "\n",
        "kmeans_model = joblib.load(os.path.join(output_dir, 'kmeans_tfidf_k5.pkl'))\n",
        "pca_model = joblib.load(os.path.join(output_dir, 'pca_tfidf_100.pkl'))\n",
        "tfidf_vectorizer = joblib.load(os.path.join(output_dir, 'vectorizer_tfidf.pkl'))\n",
        "\n",
        "def get_top_keywords_for_clusters(kmeans_model, pca_model, vectorizer, n_keywords=10):\n",
        "    centers_pca = kmeans_model.cluster_centers_\n",
        "    centers_tfidf = pca_model.inverse_transform(centers_pca)\n",
        "    terms = vectorizer.get_feature_names_out()\n",
        "\n",
        "    top_keywords = {}\n",
        "    for i, center in enumerate(centers_tfidf):\n",
        "        top_indices = center.argsort()[::-1][:n_keywords]\n",
        "        keywords = [terms[idx] for idx in top_indices]\n",
        "        top_keywords[i] = keywords\n",
        "    return top_keywords\n",
        "\n",
        "top_keywords_per_cluster = get_top_keywords_for_clusters(kmeans_model, pca_model, tfidf_vectorizer)\n",
        "\n",
        "# ==== Load recommendation data and similarity matrices ====\n",
        "df = pd.read_excel('/content/drive/My Drive/Colab/AS4/STEP4-Champion_Clustering&Classification_Save&Evaluation/Raw_with_predicted_classification_label.xlsx')\n",
        "tfidf_matrix = load_npz('/content/drive/My Drive/Colab/AS4/STEP2-feature_Engineering/tfidf_matrix.npz')\n",
        "metadata_matrix = load_npz('/content/drive/My Drive/Colab/AS4/STEP2-feature_Engineering/metadata_matrix.npz')\n",
        "\n",
        "content_similarity = cosine_similarity(tfidf_matrix)\n",
        "metadata_similarity = cosine_similarity(metadata_matrix)\n",
        "final_similarity = 0.7 * content_similarity + 0.3 * metadata_similarity\n",
        "\n",
        "def find_paper_index(title):\n",
        "    matches = df.index[df['Article Title'].str.lower() == title.lower()]\n",
        "    if len(matches) == 0:\n",
        "        return None\n",
        "    return matches[0]\n",
        "\n",
        "def recommend_papers_by_index(paper_idx, top_n=10):\n",
        "    sim_scores = final_similarity[paper_idx]\n",
        "\n",
        "    # Get top similar indices excluding self\n",
        "    top_indices = np.argsort(sim_scores)[::-1]\n",
        "    top_indices = top_indices[top_indices != paper_idx][:top_n * 5]  # broader selection\n",
        "\n",
        "    results = df.iloc[top_indices][['Article Title', 'Times Cited, All Databases', 'Author', 'Publication Year', 'Document Type', 'DOI Link']]\n",
        "    results = results.copy()\n",
        "    results['Similarity'] = sim_scores[top_indices]\n",
        "    results = results.sort_values(by='Times Cited, All Databases', ascending=False).head(top_n)\n",
        "    return results.reset_index(drop=True)\n",
        "\n",
        "# ==== Summarizer function ====\n",
        "def summarize_lsa(text, num_sentences=3):\n",
        "    parser = PlaintextParser.from_string(text, Tokenizer(\"english\"))\n",
        "    summarizer = LsaSummarizer()\n",
        "    summary = summarizer(parser.document, num_sentences)\n",
        "    if not summary:\n",
        "        return text\n",
        "    return ' '.join(str(sentence) for sentence in summary)\n",
        "\n",
        "\n",
        "# ==== Database and basic filters ====\n",
        "DATABASE = 'papers.db'\n",
        "def get_db_connection():\n",
        "    conn = sqlite3.connect(DATABASE)\n",
        "    conn.row_factory = sqlite3.Row  # to get dict-like rows\n",
        "    return conn\n",
        "\n",
        "# ==== Flask app setup ====\n",
        "app = Flask(__name__)\n",
        "\n",
        "@app.route('/predict_category', methods=['POST'])\n",
        "def predict_category():\n",
        "    data = request.get_json(force=True)\n",
        "    text = data.get('text', '')\n",
        "    if not text:\n",
        "        return jsonify({'error': 'No text provided'}), 400\n",
        "    category = predict_paper_category(text)\n",
        "    return jsonify({'category': category})\n",
        "\n",
        "@app.route('/cluster_predict', methods=['POST'])\n",
        "def cluster_predict():\n",
        "    data = request.get_json(force=True)\n",
        "    text = data.get('text', '')\n",
        "    if not text:\n",
        "        return jsonify({'error': 'No text provided'}), 400\n",
        "\n",
        "    X_vec = tfidf_vectorizer.transform([text])\n",
        "    X_vec_reduced = pca_model.transform(X_vec.toarray())\n",
        "    cluster_id = int(kmeans_model.predict(X_vec_reduced)[0])\n",
        "    keywords = top_keywords_per_cluster.get(cluster_id, [])\n",
        "\n",
        "    return jsonify({\n",
        "        \"cluster_id\": cluster_id,\n",
        "        \"top_keywords\": keywords\n",
        "    })\n",
        "\n",
        "@app.route('/cluster_themes', methods=['GET'])\n",
        "def cluster_themes():\n",
        "    safe_keywords = {int(k): [str(word) for word in v] for k, v in top_keywords_per_cluster.items()}\n",
        "    return jsonify({\n",
        "        \"n_clusters\": len(safe_keywords),\n",
        "        \"cluster_themes\": safe_keywords\n",
        "    })\n",
        "\n",
        "@app.route('/recommend', methods=['POST'])\n",
        "def recommend():\n",
        "    data = request.get_json(force=True)\n",
        "    title = data.get('title', '').strip()\n",
        "    top_n = int(data.get('top_n', 10))\n",
        "\n",
        "    if not title:\n",
        "        return jsonify({'error': 'Missing paper title'}), 400\n",
        "\n",
        "    paper_idx = find_paper_index(title)\n",
        "    if paper_idx is None:\n",
        "        return jsonify({'error': f'Paper title \"{title}\" not found'}), 404\n",
        "\n",
        "    recs_df = recommend_papers_by_index(paper_idx, top_n)\n",
        "    recs = recs_df.to_dict(orient='records')\n",
        "\n",
        "    return jsonify({\n",
        "        'input_title': title,\n",
        "        'recommendations': recs\n",
        "    })\n",
        "\n",
        "@app.route('/summarize_abstract', methods=['POST'])\n",
        "def summarize_abstract():\n",
        "    data = request.get_json(force=True)\n",
        "    abstract = data.get('abstract', '').strip()\n",
        "    if not abstract:\n",
        "        return jsonify({'error': 'No abstract provided'}), 400\n",
        "\n",
        "    num_sentences = int(data.get('num_sentences', 3))\n",
        "    summary = summarize_lsa(abstract, num_sentences)\n",
        "    return jsonify({'summary': summary})\n",
        "\n",
        "\n",
        "@app.route('/summarize_by_title', methods=['POST'])\n",
        "def summarize_by_title():\n",
        "    data = request.get_json(force=True)\n",
        "    title = data.get('title', '').strip()\n",
        "    num_sentences = int(data.get('num_sentences', 3))\n",
        "\n",
        "    if not title:\n",
        "        return jsonify({'error': 'No paper title provided'}), 400\n",
        "\n",
        "    # Find paper index by title (case-insensitive)\n",
        "    matches = df.index[df['Article Title'].str.lower() == title.lower()]\n",
        "    if len(matches) == 0:\n",
        "        return jsonify({'error': f'Paper titled \"{title}\" not found'}), 404\n",
        "\n",
        "    paper_idx = matches[0]\n",
        "    abstract = df.loc[paper_idx, 'Abstract']\n",
        "    if not abstract or pd.isna(abstract):\n",
        "        return jsonify({'error': f'No abstract available for paper \"{title}\"'}), 404\n",
        "\n",
        "    summary = summarize_lsa(abstract, num_sentences)\n",
        "    return jsonify({\n",
        "        'title': title,\n",
        "        'summary': summary\n",
        "    })\n",
        "\n",
        "@app.route('/search_papers', methods=['GET'])\n",
        "def search_papers():\n",
        "    conn = get_db_connection()\n",
        "\n",
        "    # Allowed columns for security and validation\n",
        "    allowed_cols = {\n",
        "        'id', 'article_title', 'abstract', 'author', 'publication_year',\n",
        "        'document_type', 'keywords', 'times_cited', 'doi_link', 'predicted_classification_label'\n",
        "    }\n",
        "\n",
        "    # Get requested fields from query param, else select all\n",
        "    fields = request.args.get('fields')\n",
        "    if fields:\n",
        "        requested_cols = [col.strip() for col in fields.split(',')]\n",
        "        # Filter out invalid columns to prevent SQL injection\n",
        "        selected_cols = [col for col in requested_cols if col in allowed_cols]\n",
        "        if not selected_cols:\n",
        "            # If none valid, fallback to all columns\n",
        "            selected_cols = ['*']\n",
        "    else:\n",
        "        selected_cols = ['*']\n",
        "\n",
        "    select_clause = \", \".join(selected_cols)\n",
        "\n",
        "    # Base query and params\n",
        "    query = f\"SELECT {select_clause} FROM papers WHERE 1=1\"\n",
        "    params = []\n",
        "\n",
        "    # Dynamic filters\n",
        "    author = request.args.get('author')\n",
        "    if author:\n",
        "        query += \" AND author LIKE ?\"\n",
        "        params.append(f\"%{author}%\")  # partial match\n",
        "\n",
        "    pub_year_min = request.args.get('year_min', type=int)\n",
        "    if pub_year_min is not None:\n",
        "        query += \" AND publication_year >= ?\"\n",
        "        params.append(pub_year_min)\n",
        "\n",
        "    pub_year_max = request.args.get('year_max', type=int)\n",
        "    if pub_year_max is not None:\n",
        "        query += \" AND publication_year <= ?\"\n",
        "        params.append(pub_year_max)\n",
        "\n",
        "    document_type = request.args.get('document_type')\n",
        "    if document_type:\n",
        "        query += \" AND document_type = ?\"\n",
        "        params.append(document_type)\n",
        "\n",
        "    classification = request.args.get('classification')\n",
        "    if classification:\n",
        "        query += \" AND predicted_classification_label = ?\"\n",
        "        params.append(classification)\n",
        "\n",
        "    keywords = request.args.get('keywords')\n",
        "    if keywords:\n",
        "        query += \" AND abstract LIKE ?\"\n",
        "        params.append(f\"%{keywords}%\")\n",
        "\n",
        "    # Sorting & Limit\n",
        "    order_by = request.args.get('order_by', default='times_cited')\n",
        "    order_dir = request.args.get('order_dir', default='DESC').upper()\n",
        "    if order_dir not in ['ASC', 'DESC']:\n",
        "        order_dir = 'DESC'\n",
        "    # Safety: only allow order_by on allowed columns\n",
        "    if order_by not in allowed_cols:\n",
        "        order_by = 'times_cited'\n",
        "    query += f\" ORDER BY {order_by} {order_dir}\"\n",
        "\n",
        "    limit = request.args.get('limit', default=10, type=int)\n",
        "    query += \" LIMIT ?\"\n",
        "    params.append(limit)\n",
        "\n",
        "    # Execute query\n",
        "    rows = conn.execute(query, params).fetchall()\n",
        "    conn.close()\n",
        "\n",
        "    results = [dict(row) for row in rows]\n",
        "    return jsonify({'papers': results})\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    port = 5000\n",
        "    public_url = ngrok.connect(addr=port, domain=\"musical-evident-oyster.ngrok-free.app\")\n",
        "    print(f\"ðŸ”— ngrok tunnel URL: {public_url}\")\n",
        "    app.run(host='0.0.0.0', port=port)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "id": "iPTEVqzXd-a1",
        "outputId": "194fa4d6-5f5b-4e7b-cf4a-b75806992970"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Authtoken saved to configuration file: /root/.config/ngrok/ngrok.yml\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-12-808149840.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0mkmeans_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjoblib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'kmeans_tfidf_k5.pkl'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0mpca_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjoblib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'pca_tfidf_100.pkl'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m \u001b[0mtfidf_vectorizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjoblib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'vectorizer_tfidf.pkl'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_top_keywords_for_clusters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkmeans_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpca_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvectorizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_keywords\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/joblib/numpy_pickle.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(filename, mmap_mode, ensure_native_byte_order)\u001b[0m\n\u001b[1;32m    747\u001b[0m                 \u001b[0;31m# it has been written with. Other arrays are coerced to the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    748\u001b[0m                 \u001b[0;31m# native endianness of the host system.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 749\u001b[0;31m                 obj = _unpickle(\n\u001b[0m\u001b[1;32m    750\u001b[0m                     \u001b[0mfobj\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    751\u001b[0m                     \u001b[0mensure_native_byte_order\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mensure_native_byte_order\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/joblib/numpy_pickle.py\u001b[0m in \u001b[0;36m_unpickle\u001b[0;34m(fobj, ensure_native_byte_order, filename, mmap_mode)\u001b[0m\n\u001b[1;32m    624\u001b[0m     \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    625\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 626\u001b[0;31m         \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munpickler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    627\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0munpickler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat_mode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    628\u001b[0m             warnings.warn(\n",
            "\u001b[0;32m/usr/lib/python3.11/pickle.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1211\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mEOFError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1212\u001b[0m                 \u001b[0;32massert\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbytes_types\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1213\u001b[0;31m                 \u001b[0mdispatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1214\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0m_Stop\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mstopinst\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1215\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mstopinst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/pickle.py\u001b[0m in \u001b[0;36mload_binint2\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1286\u001b[0m     \u001b[0mdispatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mBININT1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_binint1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1287\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1288\u001b[0;31m     \u001b[0;32mdef\u001b[0m \u001b[0mload_binint2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1289\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0munpack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'<H'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1290\u001b[0m     \u001b[0mdispatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mBININT2\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_binint2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}