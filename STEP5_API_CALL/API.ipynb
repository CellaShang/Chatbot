{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNuxj7wbcQ45tUiIgIKlQlB"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CKdouGpTiWd5","executionInfo":{"status":"ok","timestamp":1752883839173,"user_tz":240,"elapsed":603,"user":{"displayName":"Yue Shang","userId":"00268558011417117003"}},"outputId":"ac8fc3f8-a34c-460b-c23b-2b58ea43ad92"},"execution_count":23,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"code","execution_count":24,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UUI4tXF4iAhF","executionInfo":{"status":"ok","timestamp":1752883851166,"user_tz":240,"elapsed":10812,"user":{"displayName":"Yue Shang","userId":"00268558011417117003"}},"outputId":"54b65823-c4ed-4e1c-867b-9abeae3fb2ef"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: flask-ngrok in /usr/local/lib/python3.11/dist-packages (0.0.25)\n","Requirement already satisfied: Flask>=0.8 in /usr/local/lib/python3.11/dist-packages (from flask-ngrok) (3.1.1)\n","Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from flask-ngrok) (2.32.3)\n","Requirement already satisfied: blinker>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from Flask>=0.8->flask-ngrok) (1.9.0)\n","Requirement already satisfied: click>=8.1.3 in /usr/local/lib/python3.11/dist-packages (from Flask>=0.8->flask-ngrok) (8.2.1)\n","Requirement already satisfied: itsdangerous>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from Flask>=0.8->flask-ngrok) (2.2.0)\n","Requirement already satisfied: jinja2>=3.1.2 in /usr/local/lib/python3.11/dist-packages (from Flask>=0.8->flask-ngrok) (3.1.6)\n","Requirement already satisfied: markupsafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from Flask>=0.8->flask-ngrok) (3.0.2)\n","Requirement already satisfied: werkzeug>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from Flask>=0.8->flask-ngrok) (3.1.3)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->flask-ngrok) (3.4.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->flask-ngrok) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->flask-ngrok) (2.4.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->flask-ngrok) (2025.7.14)\n","Requirement already satisfied: flask-ngrok in /usr/local/lib/python3.11/dist-packages (0.0.25)\n","Requirement already satisfied: pyngrok in /usr/local/lib/python3.11/dist-packages (7.2.12)\n","Requirement already satisfied: Flask>=0.8 in /usr/local/lib/python3.11/dist-packages (from flask-ngrok) (3.1.1)\n","Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from flask-ngrok) (2.32.3)\n","Requirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.11/dist-packages (from pyngrok) (6.0.2)\n","Requirement already satisfied: blinker>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from Flask>=0.8->flask-ngrok) (1.9.0)\n","Requirement already satisfied: click>=8.1.3 in /usr/local/lib/python3.11/dist-packages (from Flask>=0.8->flask-ngrok) (8.2.1)\n","Requirement already satisfied: itsdangerous>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from Flask>=0.8->flask-ngrok) (2.2.0)\n","Requirement already satisfied: jinja2>=3.1.2 in /usr/local/lib/python3.11/dist-packages (from Flask>=0.8->flask-ngrok) (3.1.6)\n","Requirement already satisfied: markupsafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from Flask>=0.8->flask-ngrok) (3.0.2)\n","Requirement already satisfied: werkzeug>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from Flask>=0.8->flask-ngrok) (3.1.3)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->flask-ngrok) (3.4.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->flask-ngrok) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->flask-ngrok) (2.4.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->flask-ngrok) (2025.7.14)\n"]}],"source":["!pip install flask-ngrok\n","!pip install flask-ngrok pyngrok"]},{"cell_type":"code","source":["pip install sumy"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GZqg_0s4giw9","executionInfo":{"status":"ok","timestamp":1752883868748,"user_tz":240,"elapsed":17556,"user":{"displayName":"Yue Shang","userId":"00268558011417117003"}},"outputId":"e3a63891-1fea-40d5-8b04-21a5146dc876"},"execution_count":25,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: sumy in /usr/local/lib/python3.11/dist-packages (0.11.0)\n","Requirement already satisfied: docopt<0.7,>=0.6.1 in /usr/local/lib/python3.11/dist-packages (from sumy) (0.6.2)\n","Requirement already satisfied: breadability>=0.1.20 in /usr/local/lib/python3.11/dist-packages (from sumy) (0.1.20)\n","Requirement already satisfied: requests>=2.7.0 in /usr/local/lib/python3.11/dist-packages (from sumy) (2.32.3)\n","Requirement already satisfied: pycountry>=18.2.23 in /usr/local/lib/python3.11/dist-packages (from sumy) (24.6.1)\n","Requirement already satisfied: nltk>=3.0.2 in /usr/local/lib/python3.11/dist-packages (from sumy) (3.9.1)\n","Requirement already satisfied: chardet in /usr/local/lib/python3.11/dist-packages (from breadability>=0.1.20->sumy) (5.2.0)\n","Requirement already satisfied: lxml>=2.0 in /usr/local/lib/python3.11/dist-packages (from breadability>=0.1.20->sumy) (5.4.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk>=3.0.2->sumy) (8.2.1)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk>=3.0.2->sumy) (1.5.1)\n","Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk>=3.0.2->sumy) (2024.11.6)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk>=3.0.2->sumy) (4.67.1)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.7.0->sumy) (3.4.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.7.0->sumy) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.7.0->sumy) (2.4.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.7.0->sumy) (2025.7.14)\n"]}]},{"cell_type":"code","source":["import nltk\n","nltk.download('punkt')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6RKEwYLdhxgd","executionInfo":{"status":"ok","timestamp":1752883868945,"user_tz":240,"elapsed":165,"user":{"displayName":"Yue Shang","userId":"00268558011417117003"}},"outputId":"88ac9b5d-b00e-40dd-e8bd-f07a61e9f241"},"execution_count":26,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":26}]},{"cell_type":"code","source":["import sqlite3\n","import pandas as pd\n","\n","# Load the full DataFrame\n","df = pd.read_excel('/content/drive/My Drive/Colab/AS4/STEP4-Champion_Clustering&Classification_Save&Evaluation/Raw_with_predicted_classification_label.xlsx')\n","\n","# Rename columns for consistency\n","df.rename(columns={\n","    'Author': 'author',\n","    'Article Title': 'article_title',\n","    'Document Type': 'document_type',\n","    'Keywords': 'keywords',\n","    'Abstract': 'abstract',\n","    'Times Cited, All Databases': 'times_cited',\n","    'Publication Year': 'publication_year',\n","    'DOI Link': 'doi_link',\n","    'predicted_Classification_label': 'predicted_classification_label'\n","}, inplace=True)\n","\n","# Clean times_cited if it's a string with commas\n","df['times_cited'] = df['times_cited'].replace(',', '', regex=True)\n","df['times_cited'] = pd.to_numeric(df['times_cited'], errors='coerce')\n","df['times_cited'] = df['times_cited'].fillna(0).astype(int)\n","\n","# Create SQLite database and table\n","conn = sqlite3.connect('papers.db')\n","c = conn.cursor()\n","\n","# Updated schema with 'keywords'\n","c.execute('''\n","CREATE TABLE IF NOT EXISTS papers (\n","    id INTEGER PRIMARY KEY AUTOINCREMENT,\n","    article_title TEXT UNIQUE,\n","    abstract TEXT,\n","    author TEXT,\n","    publication_year INTEGER,\n","    document_type TEXT,\n","    keywords TEXT,\n","    times_cited INTEGER,\n","    doi_link TEXT,\n","    predicted_classification_label TEXT\n",")\n","''')\n","\n","conn.commit()\n","\n","# Insert data using pandas to_sql\n","df.to_sql('papers', conn, if_exists='replace', index=False)\n","\n","conn.close()\n","print(\" Database created and populated with all fields.\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"A55obs0cuPLR","executionInfo":{"status":"ok","timestamp":1752883876068,"user_tz":240,"elapsed":1731,"user":{"displayName":"Yue Shang","userId":"00268558011417117003"}},"outputId":"2a013e56-c768-4755-b7de-5a819f3c0e9f"},"execution_count":27,"outputs":[{"output_type":"stream","name":"stdout","text":[" Database created and populated with all fields.\n"]}]},{"cell_type":"code","source":["from google.colab import files\n","\n","files.download('papers.db')  # Replace with your actual path if needed\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":17},"id":"XCSJBHGl1riD","executionInfo":{"status":"ok","timestamp":1752883879278,"user_tz":240,"elapsed":53,"user":{"displayName":"Yue Shang","userId":"00268558011417117003"}},"outputId":"2740979b-b8ff-4b14-a6cf-5554fb8de5a8"},"execution_count":28,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["\n","    async function download(id, filename, size) {\n","      if (!google.colab.kernel.accessAllowed) {\n","        return;\n","      }\n","      const div = document.createElement('div');\n","      const label = document.createElement('label');\n","      label.textContent = `Downloading \"${filename}\": `;\n","      div.appendChild(label);\n","      const progress = document.createElement('progress');\n","      progress.max = size;\n","      div.appendChild(progress);\n","      document.body.appendChild(div);\n","\n","      const buffers = [];\n","      let downloaded = 0;\n","\n","      const channel = await google.colab.kernel.comms.open(id);\n","      // Send a message to notify the kernel that we're ready.\n","      channel.send({})\n","\n","      for await (const message of channel.messages) {\n","        // Send a message to notify the kernel that we're ready.\n","        channel.send({})\n","        if (message.buffers) {\n","          for (const buffer of message.buffers) {\n","            buffers.push(buffer);\n","            downloaded += buffer.byteLength;\n","            progress.value = downloaded;\n","          }\n","        }\n","      }\n","      const blob = new Blob(buffers, {type: 'application/binary'});\n","      const a = document.createElement('a');\n","      a.href = window.URL.createObjectURL(blob);\n","      a.download = filename;\n","      div.appendChild(a);\n","      a.click();\n","      div.remove();\n","    }\n","  "]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["download(\"download_e307dfee-a8e9-4f76-93d4-9d830b3fc40f\", \"papers.db\", 4034560)"]},"metadata":{}}]},{"cell_type":"code","source":["# Add  real token here (already copied from ngrok dashboard)\n","!ngrok config add-authtoken 303S6vb9iu9Bj1arMHaqauC0BLJ_7AgwQ7KRfbfAoBwBPgndX\n","\n","[30]\n","1h\n","\n","from flask import Flask, request, jsonify\n","from pyngrok import ngrok\n","import joblib\n","import numpy as np\n","import os\n","import pandas as pd\n","from sklearn.metrics.pairwise import cosine_similarity\n","from scipy.sparse import load_npz\n","\n","# Sumy imports for summarization\n","from sumy.parsers.plaintext import PlaintextParser\n","from sumy.nlp.tokenizers import Tokenizer\n","from sumy.summarizers.lsa import LsaSummarizer\n","\n","# ==== Load classification model and vectorizers ====\n","MODEL_PATH = '/content/drive/My Drive/Colab/AS4/STEP4-Champion_Clustering&Classification_Save&Evaluation/naive_bayes_bow_ngram.pkl'\n","BOW_VECTORIZER_PATH = '/content/drive/My Drive/Colab/AS4/STEP4-Champion_Clustering&Classification_Save&Evaluation/bow_vectorizer.pkl'\n","NGRAM_VECTORIZER_PATH = '/content/drive/My Drive/Colab/AS4/STEP4-Champion_Clustering&Classification_Save&Evaluation/ngram2_vectorizer.pkl'\n","\n","classif_model = joblib.load(MODEL_PATH)\n","bow_vectorizer = joblib.load(BOW_VECTORIZER_PATH)\n","ngram_vectorizer = joblib.load(NGRAM_VECTORIZER_PATH)\n","\n","label_map = {\n","    0: 'Gene Expression Analysis',\n","    1: 'Sequence Classification',\n","    2: 'Protein Structure Prediction',\n","    3: 'Biological Image Analysis',\n","    4: 'Disease Outcome Prediction'\n","}\n","\n","def predict_paper_category(text):\n","    bow_features = bow_vectorizer.transform([text])\n","    ngram_features = ngram_vectorizer.transform([text])\n","    X = np.hstack([bow_features.toarray(), ngram_features.toarray()])\n","    label_num = classif_model.predict(X)[0]\n","    return label_map[label_num]\n","\n","# ==== Load clustering model, PCA, vectorizer ====\n","output_dir = '/content/drive/My Drive/Colab/AS4/STEP4-Champion_Clustering&Classification_Save&Evaluation'\n","\n","kmeans_model = joblib.load(os.path.join(output_dir, 'kmeans_tfidf_k5.pkl'))\n","pca_model = joblib.load(os.path.join(output_dir, 'pca_tfidf_100.pkl'))\n","tfidf_vectorizer = joblib.load(os.path.join(output_dir, 'vectorizer_tfidf.pkl'))\n","\n","def get_top_keywords_for_clusters(kmeans_model, pca_model, vectorizer, n_keywords=10):\n","    centers_pca = kmeans_model.cluster_centers_\n","    centers_tfidf = pca_model.inverse_transform(centers_pca)\n","    terms = vectorizer.get_feature_names_out()\n","\n","    top_keywords = {}\n","    for i, center in enumerate(centers_tfidf):\n","        top_indices = center.argsort()[::-1][:n_keywords]\n","        keywords = [terms[idx] for idx in top_indices]\n","        top_keywords[i] = keywords\n","    return top_keywords\n","\n","top_keywords_per_cluster = get_top_keywords_for_clusters(kmeans_model, pca_model, tfidf_vectorizer)\n","\n","# ==== Load recommendation data and similarity matrices ====\n","df = pd.read_excel('/content/drive/My Drive/Colab/AS4/STEP4-Champion_Clustering&Classification_Save&Evaluation/Raw_with_predicted_classification_label.xlsx')\n","tfidf_matrix = load_npz('/content/drive/My Drive/Colab/AS4/STEP2-feature_Engineering/tfidf_matrix.npz')\n","metadata_matrix = load_npz('/content/drive/My Drive/Colab/AS4/STEP2-feature_Engineering/metadata_matrix.npz')\n","\n","content_similarity = cosine_similarity(tfidf_matrix)\n","metadata_similarity = cosine_similarity(metadata_matrix)\n","final_similarity = 0.7 * content_similarity + 0.3 * metadata_similarity\n","\n","def find_paper_index(title):\n","    matches = df.index[df['Article Title'].str.lower() == title.lower()]\n","    if len(matches) == 0:\n","        return None\n","    return matches[0]\n","\n","def recommend_papers_by_index(paper_idx, top_n=10):\n","    sim_scores = final_similarity[paper_idx]\n","\n","    # Get top similar indices excluding self\n","    top_indices = np.argsort(sim_scores)[::-1]\n","    top_indices = top_indices[top_indices != paper_idx][:top_n * 5]  # broader selection\n","\n","    results = df.iloc[top_indices][['Article Title', 'Times Cited, All Databases', 'Author', 'Publication Year', 'Document Type', 'DOI Link']]\n","    results = results.copy()\n","    results['Similarity'] = sim_scores[top_indices]\n","    results = results.sort_values(by='Times Cited, All Databases', ascending=False).head(top_n)\n","    return results.reset_index(drop=True)\n","\n","# ==== Summarizer function ====\n","def summarize_lsa(text, num_sentences=3):\n","    parser = PlaintextParser.from_string(text, Tokenizer(\"english\"))\n","    summarizer = LsaSummarizer()\n","    summary = summarizer(parser.document, num_sentences)\n","    if not summary:\n","        return text\n","    return ' '.join(str(sentence) for sentence in summary)\n","\n","\n","# ==== Database and basic filters ====\n","DATABASE = 'papers.db'\n","def get_db_connection():\n","    conn = sqlite3.connect(DATABASE)\n","    conn.row_factory = sqlite3.Row  # to get dict-like rows\n","    return conn\n","\n","# ==== Flask app setup ====\n","app = Flask(__name__)\n","\n","@app.route('/predict_category', methods=['POST'])\n","def predict_category():\n","    data = request.get_json(force=True)\n","    text = data.get('text', '')\n","    if not text:\n","        return jsonify({'error': 'No text provided'}), 400\n","    category = predict_paper_category(text)\n","    return jsonify({'category': category})\n","\n","@app.route('/cluster_predict', methods=['POST'])\n","def cluster_predict():\n","    data = request.get_json(force=True)\n","    text = data.get('text', '')\n","    if not text:\n","        return jsonify({'error': 'No text provided'}), 400\n","\n","    X_vec = tfidf_vectorizer.transform([text])\n","    X_vec_reduced = pca_model.transform(X_vec.toarray())\n","    cluster_id = int(kmeans_model.predict(X_vec_reduced)[0])\n","    keywords = top_keywords_per_cluster.get(cluster_id, [])\n","\n","    return jsonify({\n","        \"cluster_id\": cluster_id,\n","        \"top_keywords\": keywords\n","    })\n","\n","@app.route('/cluster_themes', methods=['GET'])\n","def cluster_themes():\n","    safe_keywords = {int(k): [str(word) for word in v] for k, v in top_keywords_per_cluster.items()}\n","    return jsonify({\n","        \"n_clusters\": len(safe_keywords),\n","        \"cluster_themes\": safe_keywords\n","    })\n","\n","@app.route('/recommend', methods=['POST'])\n","def recommend():\n","    data = request.get_json(force=True)\n","    title = data.get('title', '').strip()\n","    top_n = int(data.get('top_n', 10))\n","\n","    if not title:\n","        return jsonify({'error': 'Missing paper title'}), 400\n","\n","    paper_idx = find_paper_index(title)\n","    if paper_idx is None:\n","        return jsonify({'error': f'Paper title \"{title}\" not found'}), 404\n","\n","    recs_df = recommend_papers_by_index(paper_idx, top_n)\n","    recs = recs_df.to_dict(orient='records')\n","\n","    return jsonify({\n","        'input_title': title,\n","        'recommendations': recs\n","    })\n","\n","@app.route('/summarize_abstract', methods=['POST'])\n","def summarize_abstract():\n","    data = request.get_json(force=True)\n","    abstract = data.get('abstract', '').strip()\n","    if not abstract:\n","        return jsonify({'error': 'No abstract provided'}), 400\n","\n","    num_sentences = int(data.get('num_sentences', 3))\n","    summary = summarize_lsa(abstract, num_sentences)\n","    return jsonify({'summary': summary})\n","\n","\n","@app.route('/summarize_by_title', methods=['POST'])\n","def summarize_by_title():\n","    data = request.get_json(force=True)\n","    title = data.get('title', '').strip()\n","    num_sentences = int(data.get('num_sentences', 3))\n","\n","    if not title:\n","        return jsonify({'error': 'No paper title provided'}), 400\n","\n","    # Find paper index by title (case-insensitive)\n","    matches = df.index[df['Article Title'].str.lower() == title.lower()]\n","    if len(matches) == 0:\n","        return jsonify({'error': f'Paper titled \"{title}\" not found'}), 404\n","\n","    paper_idx = matches[0]\n","    abstract = df.loc[paper_idx, 'Abstract']\n","    if not abstract or pd.isna(abstract):\n","        return jsonify({'error': f'No abstract available for paper \"{title}\"'}), 404\n","\n","    summary = summarize_lsa(abstract, num_sentences)\n","    return jsonify({\n","        'title': title,\n","        'summary': summary\n","    })\n","\n","@app.route('/search_papers', methods=['GET'])\n","def search_papers():\n","    conn = get_db_connection()\n","\n","    # Allowed columns for security and validation\n","    allowed_cols = {\n","        'id', 'article_title', 'abstract', 'author', 'publication_year',\n","        'document_type', 'keywords', 'times_cited', 'doi_link', 'predicted_classification_label'\n","    }\n","\n","    # Get requested fields from query param, else select all\n","    fields = request.args.get('fields')\n","    if fields:\n","        requested_cols = [col.strip() for col in fields.split(',')]\n","        # Filter out invalid columns to prevent SQL injection\n","        selected_cols = [col for col in requested_cols if col in allowed_cols]\n","        if not selected_cols:\n","            # If none valid, fallback to all columns\n","            selected_cols = ['*']\n","    else:\n","        selected_cols = ['*']\n","\n","    select_clause = \", \".join(selected_cols)\n","\n","    # Base query and params\n","    query = f\"SELECT {select_clause} FROM papers WHERE 1=1\"\n","    params = []\n","\n","    # Dynamic filters\n","    author = request.args.get('author')\n","    if author:\n","        query += \" AND author LIKE ?\"\n","        params.append(f\"%{author}%\")  # partial match\n","\n","    pub_year_min = request.args.get('year_min', type=int)\n","    if pub_year_min is not None:\n","        query += \" AND publication_year >= ?\"\n","        params.append(pub_year_min)\n","\n","    pub_year_max = request.args.get('year_max', type=int)\n","    if pub_year_max is not None:\n","        query += \" AND publication_year <= ?\"\n","        params.append(pub_year_max)\n","\n","    document_type = request.args.get('document_type')\n","    if document_type:\n","        query += \" AND document_type = ?\"\n","        params.append(document_type)\n","\n","    classification = request.args.get('classification')\n","    if classification:\n","        query += \" AND predicted_classification_label = ?\"\n","        params.append(classification)\n","\n","    keywords = request.args.get('keywords')\n","    if keywords:\n","        query += \" AND abstract LIKE ?\"\n","        params.append(f\"%{keywords}%\")\n","\n","    # Sorting & Limit\n","    order_by = request.args.get('order_by', default='times_cited')\n","    order_dir = request.args.get('order_dir', default='DESC').upper()\n","    if order_dir not in ['ASC', 'DESC']:\n","        order_dir = 'DESC'\n","    # Safety: only allow order_by on allowed columns\n","    if order_by not in allowed_cols:\n","        order_by = 'times_cited'\n","    query += f\" ORDER BY {order_by} {order_dir}\"\n","\n","    limit = request.args.get('limit', default=10, type=int)\n","    query += \" LIMIT ?\"\n","    params.append(limit)\n","\n","    # Execute query\n","    rows = conn.execute(query, params).fetchall()\n","    conn.close()\n","\n","    results = [dict(row) for row in rows]\n","    return jsonify({'papers': results})\n","\n","if __name__ == '__main__':\n","    port = 5000\n","    public_url = ngrok.connect(port)\n","    print(f\"🔗 ngrok tunnel URL: {public_url}\")\n","    app.run(host='0.0.0.0', port=port)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"iPTEVqzXd-a1","executionInfo":{"status":"ok","timestamp":1752890390454,"user_tz":240,"elapsed":6454855,"user":{"displayName":"Yue Shang","userId":"00268558011417117003"}},"outputId":"ddb52907-6a5b-4559-f2df-53c6cfd636f6"},"execution_count":30,"outputs":[{"output_type":"stream","name":"stdout","text":["Authtoken saved to configuration file: /root/.config/ngrok/ngrok.yml\n","🔗 ngrok tunnel URL: NgrokTunnel: \"https://6c0d26ac3a12.ngrok-free.app\" -> \"http://localhost:5000\"\n"," * Serving Flask app '__main__'\n"," * Debug mode: off\n"]},{"output_type":"stream","name":"stderr","text":["INFO:werkzeug:\u001b[31m\u001b[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\u001b[0m\n"," * Running on all addresses (0.0.0.0)\n"," * Running on http://127.0.0.1:5000\n"," * Running on http://172.28.0.12:5000\n","INFO:werkzeug:\u001b[33mPress CTRL+C to quit\u001b[0m\n","INFO:werkzeug:127.0.0.1 - - [19/Jul/2025 00:25:50] \"GET /search_papers?limit=3 HTTP/1.1\" 200 -\n","INFO:werkzeug:127.0.0.1 - - [19/Jul/2025 00:25:51] \"\u001b[33mGET /favicon.ico HTTP/1.1\u001b[0m\" 404 -\n","INFO:werkzeug:127.0.0.1 - - [19/Jul/2025 00:26:41] \"\u001b[33mGET /fields=article_title,author&limit=2 HTTP/1.1\u001b[0m\" 404 -\n","INFO:werkzeug:127.0.0.1 - - [19/Jul/2025 00:26:58] \"GET /search_papers?fields=article_title,author&limit=2 HTTP/1.1\" 200 -\n","INFO:werkzeug:127.0.0.1 - - [19/Jul/2025 01:14:54] \"GET /search_papers?fields=article_title,author&author=Smith&document_type=view&order_by=times_cited&order_dir=DESC&limit=3 HTTP/1.1\" 200 -\n","INFO:werkzeug:127.0.0.1 - - [19/Jul/2025 01:14:59] \"GET /search_papers?fields=article_title,author&author=Smith&document_type=view&order_by=times_cited&order_dir=DESC&limit=3 HTTP/1.1\" 200 -\n","INFO:werkzeug:127.0.0.1 - - [19/Jul/2025 01:19:18] \"GET /search_papers?fields=article_title,author&document_type=review&classification=Gene%20Expression%20Analysis&order_by=times_cited&order_dir=DESC&limit=3 HTTP/1.1\" 200 -\n","INFO:werkzeug:127.0.0.1 - - [19/Jul/2025 01:19:22] \"GET /search_papers?fields=article_title,author&document_type=review&classification=Gene%20Expression%20Analysis&order_by=times_cited&order_dir=DESC&limit=3 HTTP/1.1\" 200 -\n","INFO:werkzeug:127.0.0.1 - - [19/Jul/2025 01:19:25] \"GET /search_papers?fields=article_title,author&document_type=review&classification=Gene%20Expression%20Analysis&order_by=times_cited&order_dir=DESC&limit=3 HTTP/1.1\" 200 -\n","INFO:werkzeug:127.0.0.1 - - [19/Jul/2025 01:19:58] \"GET /search_papers?fields=article_title,author&document_type=review&classification=Gene%20Expression%20Analysis&order_by=times_cited&order_dir=DESC&limit=3 HTTP/1.1\" 200 -\n","INFO:werkzeug:127.0.0.1 - - [19/Jul/2025 01:22:01] \"GET /search_papers?fields=article_title,author&document_type=Review&classification=gene%20expression%20analysis&order_by=times_cited&order_dir=DESC&limit=3 HTTP/1.1\" 200 -\n","INFO:werkzeug:127.0.0.1 - - [19/Jul/2025 01:24:44] \"GET /search_papers?fields=article_title,author&document_type=Article&year_min=2010&limit=10 HTTP/1.1\" 200 -\n"]}]},{"cell_type":"code","source":["flask_code = \"\"\"\n","from flask import Flask, request, jsonify\n","from pyngrok import ngrok\n","import joblib\n","import numpy as np\n","import os\n","import pandas as pd\n","from sklearn.metrics.pairwise import cosine_similarity\n","from scipy.sparse import load_npz\n","from sumy.parsers.plaintext import PlaintextParser\n","from sumy.nlp.tokenizers import Tokenizer\n","from sumy.summarizers.lsa import LsaSummarizer\n","import sqlite3\n","\n","# ==== Load classification model and vectorizers ====\n","MODEL_PATH = '/content/drive/My Drive/Colab/AS4/STEP4-Champion_Clustering&Classification_Save&Evaluation/naive_bayes_bow_ngram.pkl'\n","BOW_VECTORIZER_PATH = '/content/drive/My Drive/Colab/AS4/STEP4-Champion_Clustering&Classification_Save&Evaluation/bow_vectorizer.pkl'\n","NGRAM_VECTORIZER_PATH = '/content/drive/My Drive/Colab/AS4/STEP4-Champion_Clustering&Classification_Save&Evaluation/ngram2_vectorizer.pkl'\n","\n","classif_model = joblib.load(MODEL_PATH)\n","bow_vectorizer = joblib.load(BOW_VECTORIZER_PATH)\n","ngram_vectorizer = joblib.load(NGRAM_VECTORIZER_PATH)\n","\n","label_map = {\n","    0: 'Gene Expression Analysis',\n","    1: 'Sequence Classification',\n","    2: 'Protein Structure Prediction',\n","    3: 'Biological Image Analysis',\n","    4: 'Disease Outcome Prediction'\n","}\n","\n","def predict_paper_category(text):\n","    bow_features = bow_vectorizer.transform([text])\n","    ngram_features = ngram_vectorizer.transform([text])\n","    X = np.hstack([bow_features.toarray(), ngram_features.toarray()])\n","    label_num = classif_model.predict(X)[0]\n","    return label_map[label_num]\n","\n","# ==== Load clustering model, PCA, vectorizer ====\n","output_dir = '/content/drive/My Drive/Colab/AS4/STEP4-Champion_Clustering&Classification_Save&Evaluation'\n","\n","kmeans_model = joblib.load(os.path.join(output_dir, 'kmeans_tfidf_k5.pkl'))\n","pca_model = joblib.load(os.path.join(output_dir, 'pca_tfidf_100.pkl'))\n","tfidf_vectorizer = joblib.load(os.path.join(output_dir, 'vectorizer_tfidf.pkl'))\n","\n","def get_top_keywords_for_clusters(kmeans_model, pca_model, vectorizer, n_keywords=10):\n","    centers_pca = kmeans_model.cluster_centers_\n","    centers_tfidf = pca_model.inverse_transform(centers_pca)\n","    terms = vectorizer.get_feature_names_out()\n","\n","    top_keywords = {}\n","    for i, center in enumerate(centers_tfidf):\n","        top_indices = center.argsort()[::-1][:n_keywords]\n","        keywords = [terms[idx] for idx in top_indices]\n","        top_keywords[i] = keywords\n","    return top_keywords\n","\n","top_keywords_per_cluster = get_top_keywords_for_clusters(kmeans_model, pca_model, tfidf_vectorizer)\n","\n","# ==== Load recommendation data and similarity matrices ====\n","df = pd.read_excel('/content/drive/My Drive/Colab/AS4/STEP4-Champion_Clustering&Classification_Save&Evaluation/Raw_with_predicted_classification_label.xlsx')\n","tfidf_matrix = load_npz('/content/drive/My Drive/Colab/AS4/STEP2-feature_Engineering/tfidf_matrix.npz')\n","metadata_matrix = load_npz('/content/drive/My Drive/Colab/AS4/STEP2-feature_Engineering/metadata_matrix.npz')\n","\n","content_similarity = cosine_similarity(tfidf_matrix)\n","metadata_similarity = cosine_similarity(metadata_matrix)\n","final_similarity = 0.7 * content_similarity + 0.3 * metadata_similarity\n","\n","def find_paper_index(title):\n","    matches = df.index[df['Article Title'].str.lower() == title.lower()]\n","    if len(matches) == 0:\n","        return None\n","    return matches[0]\n","\n","def recommend_papers_by_index(paper_idx, top_n=10):\n","    sim_scores = final_similarity[paper_idx]\n","    top_indices = np.argsort(sim_scores)[::-1]\n","    top_indices = top_indices[top_indices != paper_idx][:top_n * 5]\n","\n","    results = df.iloc[top_indices][['Article Title', 'Times Cited, All Databases', 'Author', 'Publication Year', 'Document Type', 'DOI Link']]\n","    results = results.copy()\n","    results['Similarity'] = sim_scores[top_indices]\n","    results = results.sort_values(by='Times Cited, All Databases', ascending=False).head(top_n)\n","    return results.reset_index(drop=True)\n","\n","def summarize_lsa(text, num_sentences=3):\n","    parser = PlaintextParser.from_string(text, Tokenizer(\"english\"))\n","    summarizer = LsaSummarizer()\n","    summary = summarizer(parser.document, num_sentences)\n","    if not summary:\n","        return text\n","    return ' '.join(str(sentence) for sentence in summary)\n","\n","DATABASE = 'papers.db'\n","def get_db_connection():\n","    conn = sqlite3.connect(DATABASE)\n","    conn.row_factory = sqlite3.Row\n","    return conn\n","\n","app = Flask(__name__)\n","\n","@app.route('/predict_category', methods=['POST'])\n","def predict_category():\n","    data = request.get_json(force=True)\n","    text = data.get('text', '')\n","    if not text:\n","        return jsonify({'error': 'No text provided'}), 400\n","    category = predict_paper_category(text)\n","    return jsonify({'category': category})\n","\n","@app.route('/cluster_predict', methods=['POST'])\n","def cluster_predict():\n","    data = request.get_json(force=True)\n","    text = data.get('text', '')\n","    if not text:\n","        return jsonify({'error': 'No text provided'}), 400\n","\n","    X_vec = tfidf_vectorizer.transform([text])\n","    X_vec_reduced = pca_model.transform(X_vec.toarray())\n","    cluster_id = int(kmeans_model.predict(X_vec_reduced)[0])\n","    keywords = top_keywords_per_cluster.get(cluster_id, [])\n","\n","    return jsonify({\n","        \"cluster_id\": cluster_id,\n","        \"top_keywords\": keywords\n","    })\n","\n","@app.route('/cluster_themes', methods=['GET'])\n","def cluster_themes():\n","    safe_keywords = {int(k): [str(word) for word in v] for k, v in top_keywords_per_cluster.items()}\n","    return jsonify({\n","        \"n_clusters\": len(safe_keywords),\n","        \"cluster_themes\": safe_keywords\n","    })\n","\n","@app.route('/recommend', methods=['POST'])\n","def recommend():\n","    data = request.get_json(force=True)\n","    title = data.get('title', '').strip()\n","    top_n = int(data.get('top_n', 10))\n","\n","    if not title:\n","        return jsonify({'error': 'Missing paper title'}), 400\n","\n","    paper_idx = find_paper_index(title)\n","    if paper_idx is None:\n","        return jsonify({'error': f'Paper title \"{title}\" not found'}), 404\n","\n","    recs_df = recommend_papers_by_index(paper_idx, top_n)\n","    recs = recs_df.to_dict(orient='records')\n","\n","    return jsonify({\n","        'input_title': title,\n","        'recommendations': recs\n","    })\n","\n","@app.route('/summarize_abstract', methods=['POST'])\n","def summarize_abstract():\n","    data = request.get_json(force=True)\n","    abstract = data.get('abstract', '').strip()\n","    if not abstract:\n","        return jsonify({'error': 'No abstract provided'}), 400\n","\n","    num_sentences = int(data.get('num_sentences', 3))\n","    summary = summarize_lsa(abstract, num_sentences)\n","    return jsonify({'summary': summary})\n","\n","@app.route('/summarize_by_title', methods=['POST'])\n","def summarize_by_title():\n","    data = request.get_json(force=True)\n","    title = data.get('title', '').strip()\n","    num_sentences = int(data.get('num_sentences', 3))\n","\n","    if not title:\n","        return jsonify({'error': 'No paper title provided'}), 400\n","\n","    matches = df.index[df['Article Title'].str.lower() == title.lower()]\n","    if len(matches) == 0:\n","        return jsonify({'error': f'Paper titled \"{title}\" not found'}), 404\n","\n","    paper_idx = matches[0]\n","    abstract = df.loc[paper_idx, 'Abstract']\n","    if not abstract or pd.isna(abstract):\n","        return jsonify({'error': f'No abstract available for paper \"{title}\"'}), 404\n","\n","    summary = summarize_lsa(abstract, num_sentences)\n","    return jsonify({\n","        'title': title,\n","        'summary': summary\n","    })\n","\n","@app.route('/search_papers', methods=['GET'])\n","def search_papers():\n","    conn = get_db_connection()\n","\n","    allowed_cols = {\n","        'id', 'article_title', 'abstract', 'author', 'publication_year',\n","        'document_type', 'keywords', 'times_cited', 'doi_link', 'predicted_classification_label'\n","    }\n","\n","    fields = request.args.get('fields')\n","    if fields:\n","        requested_cols = [col.strip() for col in fields.split(',')]\n","        selected_cols = [col for col in requested_cols if col in allowed_cols]\n","        if not selected_cols:\n","            selected_cols = ['*']\n","    else:\n","        selected_cols = ['*']\n","\n","    select_clause = \", \".join(selected_cols)\n","\n","    query = f\"SELECT {select_clause} FROM papers WHERE 1=1\"\n","    params = []\n","\n","    author = request.args.get('author')\n","    if author:\n","        query += \" AND author LIKE ?\"\n","        params.append(f\"%{author}%\")\n","\n","    pub_year_min = request.args.get('year_min', type=int)\n","    if pub_year_min is not None:\n","        query += \" AND publication_year >= ?\"\n","        params.append(pub_year_min)\n","\n","    pub_year_max = request.args.get('year_max', type=int)\n","    if pub_year_max is not None:\n","        query += \" AND publication_year <= ?\"\n","        params.append(pub_year_max)\n","\n","    document_type = request.args.get('document_type')\n","    if document_type:\n","        query += \" AND document_type = ?\"\n","        params.append(document_type)\n","\n","    classification = request.args.get('classification')\n","    if classification:\n","        query += \" AND predicted_classification_label = ?\"\n","        params.append(classification)\n","\n","    keywords = request.args.get('keywords')\n","    if keywords:\n","        query += \" AND abstract LIKE ?\"\n","        params.append(f\"%{keywords}%\")\n","\n","    order_by = request.args.get('order_by', default='times_cited')\n","    order_dir = request.args.get('order_dir', default='DESC').upper()\n","    if order_dir not in ['ASC', 'DESC']:\n","        order_dir = 'DESC'\n","    if order_by not in allowed_cols:\n","        order_by = 'times_cited'\n","    query += f\" ORDER BY {order_by} {order_dir}\"\n","\n","    limit = request.args.get('limit', default=10, type=int)\n","    query += \" LIMIT ?\"\n","    params.append(limit)\n","\n","    rows = conn.execute(query, params).fetchall()\n","    conn.close()\n","\n","    results = [dict(row) for row in rows]\n","    return jsonify({'papers': results})\n","\n","if __name__ == '__main__':\n","    port = 5000\n","    public_url = ngrok.connect(port)\n","    print(f\"🔗 ngrok tunnel URL: {public_url}\")\n","    app.run(host='0.0.0.0', port=port)\n","\"\"\"\n","\n","with open('/content/drive/MyDrive/Colab/AS4/your_flask_app.py', 'w') as f:\n","    f.write(flask_code)\n","\n","print(\"Flask app saved as your_flask_app.py in your Google Drive folder.\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"f2GKDBd7aKzK","executionInfo":{"status":"ok","timestamp":1752890399508,"user_tz":240,"elapsed":18,"user":{"displayName":"Yue Shang","userId":"00268558011417117003"}},"outputId":"27aacbcc-db74-4b6f-c62f-caaacd105fd0"},"execution_count":31,"outputs":[{"output_type":"stream","name":"stdout","text":["Flask app saved as your_flask_app.py in your Google Drive folder.\n"]}]}]}