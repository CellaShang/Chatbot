{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyN3lo4OLm4H+ZkkmtO2vrF+"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WTYpc7NMIgUQ","executionInfo":{"status":"ok","timestamp":1752866413022,"user_tz":240,"elapsed":13916,"user":{"displayName":"Yue Shang","userId":"00268558011417117003"}},"outputId":"52705356-2cb1-4c5f-badf-8b5b3cea9ef3"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["# ============================\n","# Step 0: Setup & Preprocessing\n","# ============================\n","import pandas as pd\n","import random\n","import re\n","import nltk\n","from nltk.corpus import stopwords\n","from nltk.stem import WordNetLemmatizer\n","\n","# Download resources if not already\n","nltk.download('stopwords')\n","nltk.download('wordnet')\n","nltk.download('omw-1.4')\n","\n","# Initialize\n","stop_words = set(stopwords.words('english'))\n","lemmatizer = WordNetLemmatizer()\n","\n","# ============================\n","# Step 1: Text Cleaning Utilities\n","# ============================\n","def clean_text(text):\n","    if not isinstance(text, str):\n","        return \"\"\n","    text = text.lower()\n","    text = re.sub(r'[^a-z\\s]', '', text)\n","    words = text.split()\n","    words = [w for w in words if w not in stop_words]\n","    words = [lemmatizer.lemmatize(w) for w in words]\n","    return ' '.join(words)\n","\n","def truncate_text(text, target_words=150):\n","    words = text.split()\n","    if len(words) >= target_words:\n","        start = random.randint(0, len(words) - target_words)\n","        return ' '.join(words[start:start + target_words])\n","    else:\n","        return text\n","\n","# ============================\n","# Step 2: Main Processing Function\n","# ============================\n","def prepare_final_dataset(excel_paths, file_labels, samples_per_file=200, words_per_doc=150):\n","    all_records = []\n","\n","    for path, file_label in zip(excel_paths, file_labels):\n","        df = pd.read_excel(path)\n","        df_sampled = df.sample(n=min(samples_per_file, len(df)), random_state=42)\n","\n","        for _, row in df_sampled.iterrows():\n","            combined_text = f\"{row.get('Article Title', '')} {row.get('Abstract', '')}\"\n","            cleaned = clean_text(combined_text)\n","            final_text = truncate_text(cleaned, target_words=words_per_doc)\n","\n","            # Copy original row to dict, then add processed fields\n","            record = row.to_dict()\n","            record['Text'] = final_text\n","            record['Label'] = file_label\n","\n","            all_records.append(record)\n","\n","    return pd.DataFrame(all_records)\n","\n","# ============================\n","# Step 3: Execute the Pipeline\n","# ============================\n","excel_files = [f'/content/drive/My Drive/Colab/AS4/dataset-{i}.xlsx' for i in range(1, 6)]\n","file_labels = [\n","    'gene expression analysis',\n","    'sequence classification and alignment',\n","    'protein structure and function prediction',\n","    'biological image analysis',\n","    'disease outcome prediction'\n","]\n","\n","df_final = prepare_final_dataset(excel_files, file_labels)\n","\n","# ============================\n","# Step 4: Export only one final file\n","# ============================\n","output_path_final = '/content/drive/My Drive/Colab/AS4/STEP1-data_prepare/final_labeled_dataset.xlsx'\n","df_final.to_excel(output_path_final, index=False)\n","print(f\"Final dataset saved to:\\n{output_path_final}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zr06EO5O-suE","executionInfo":{"status":"ok","timestamp":1752866440748,"user_tz":240,"elapsed":12436,"user":{"displayName":"Yue Shang","userId":"00268558011417117003"}},"outputId":"f43fb46b-65b3-4e85-e4aa-207ae389a7d4"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n","[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n"]},{"output_type":"stream","name":"stdout","text":["Final dataset saved to:\n","/content/drive/My Drive/Colab/AS4/STEP1-data_prepare/final_labeled_dataset.xlsx\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_GtkVIYPHfQi","executionInfo":{"status":"ok","timestamp":1752718952151,"user_tz":240,"elapsed":5576,"user":{"displayName":"Yue Shang","userId":"00268558011417117003"}},"outputId":"436382d4-ba53-4e26-b820-8b083c687c50"},"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data]   Package wordnet is already up-to-date!\n","[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n","[nltk_data]   Package omw-1.4 is already up-to-date!\n"]},{"output_type":"stream","name":"stdout","text":["Metadata & Processed Dataset Saved: /content/drive/My Drive/Colab/AS4/STEP1-data_prepare/Validation_Metadata.xlsx\n","Exported Cleaned Dataset: /content/drive/My Drive/Colab/AS4/STEP1-data_prepare/final_labeled_dataset.xlsx\n"]}],"source":["# ============================\n","# Step 0: Setup & Preprocessing\n","# ============================\n","import pandas as pd\n","import random\n","import re\n","import nltk\n","from nltk.corpus import stopwords\n","from nltk.stem import WordNetLemmatizer\n","\n","# Download resources if not already\n","nltk.download('stopwords')\n","nltk.download('wordnet')\n","nltk.download('omw-1.4')\n","\n","# Initialize\n","stop_words = set(stopwords.words('english'))\n","lemmatizer = WordNetLemmatizer()\n","\n","# ============================\n","# Step 1: Text Cleaning Utilities\n","# ============================\n","def clean_text(text):\n","    if not isinstance(text, str):\n","        return \"\"\n","    text = text.lower()\n","    text = re.sub(r'[^a-z\\s]', '', text)\n","    words = text.split()\n","    words = [w for w in words if w not in stop_words]\n","    words = [lemmatizer.lemmatize(w) for w in words]\n","    return ' '.join(words)\n","\n","def truncate_text(text, target_words=150):\n","    words = text.split()\n","    if len(words) >= target_words:\n","        start = random.randint(0, len(words) - target_words)\n","        return ' '.join(words[start:start + target_words])\n","    else:\n","        return text\n","\n","# ============================\n","# Step 2: Main Processing Function\n","# ============================\n","def prepare_labeled_dataset(excel_paths, file_labels, samples_per_file=200, words_per_doc=150):\n","    all_records = []\n","\n","    for path, file_label in zip(excel_paths, file_labels):\n","        df = pd.read_excel(path)\n","        df_sampled = df.sample(n=min(samples_per_file, len(df)), random_state=42)\n","\n","        for _, row in df_sampled.iterrows():\n","            combined_text = f\"{row['Article Title']} {row['Abstract']}\"\n","            cleaned = clean_text(combined_text)\n","            processed = truncate_text(cleaned, target_words=words_per_doc)\n","\n","            record = {\n","                'Author': row.get('Author', ''),\n","                'Article Title': row.get('Article Title', ''),\n","                'Document Type': row.get('Document Type', ''),\n","                'Keywords': row.get('Keywords', ''),\n","                'Abstract': row.get('Abstract', ''),\n","                'Times Cited': row.get('Times Cited', ''),\n","                'Publication Year': row.get('Publication Year', ''),\n","                'DOI Link': row.get('DOI Link', ''),\n","                'Original Label': row.get('Label', ''),\n","                'combined_text': combined_text,\n","                'cleaned': cleaned,\n","                'Final_Text': processed,\n","                'Label_with_Code': file_label,\n","                'word_count': len(processed.split())\n","            }\n","\n","            all_records.append(record)\n","\n","    return pd.DataFrame(all_records)\n","\n","# ============================\n","# Step 3: Execute the Pipeline\n","# ============================\n","excel_files = [f'/content/drive/My Drive/Colab/AS4/dataset-{i}.xlsx' for i in range(1, 6)]\n","file_labels = [\n","    'gene expression analysis',\n","    'sequence classification and alignment',\n","    'protein structure and function prediction',\n","    'biological image analysis',\n","    'disease outcome prediction'\n","]\n","\n","# ============================\n","# Step 4: Export full labeled dataset\n","# ============================\n","df_labeled = prepare_labeled_dataset(excel_files, file_labels)\n","output_path_full = '/content/drive/My Drive/Colab/AS4/STEP1-data_prepare/Validation_Metadata.xlsx'\n","df_labeled.to_excel(output_path_full, index=False)\n","print(f\"Metadata & Processed Dataset Saved: {output_path_full}\")\n","\n","# ============================\n","# Step 5: Export Cleaned Final Text + Label\n","# ============================\n","df_final_export = df_labeled[['Final_Text', 'Label_with_Code']].rename(columns={\n","    'Final_Text': 'Text',\n","    'Label_with_Code': 'Label'\n","})\n","\n","output_path_final = '/content/drive/My Drive/Colab/AS4/STEP1-data_prepare/final_labeled_dataset.xlsx'\n","df_final_export.to_excel(output_path_final, index=False)\n","print(f\"Exported Cleaned Dataset: {output_path_final}\")\n"]}]}