{"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"popMbQluSkji","executionInfo":{"status":"ok","timestamp":1752867895167,"user_tz":240,"elapsed":12635,"user":{"displayName":"Yue Shang","userId":"00268558011417117003"}},"outputId":"b7e0aec7-7824-482f-b8f9-e03503bc7eb3"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":984},"id":"xqgd6k9eoYpb","outputId":"6391dcae-5279-478d-efd8-4ba4a55a2947","executionInfo":{"status":"ok","timestamp":1752849431854,"user_tz":240,"elapsed":31447,"user":{"displayName":"Yue Shang","userId":"00268558011417117003"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting gensim\n","  Downloading gensim-4.3.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.1 kB)\n","Collecting numpy<2.0,>=1.18.5 (from gensim)\n","  Downloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting scipy<1.14.0,>=1.7.0 (from gensim)\n","  Downloading scipy-1.13.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.6/60.6 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting smart-open>=1.8.1 (from gensim)\n","  Downloading smart_open-7.3.0.post1-py3-none-any.whl.metadata (24 kB)\n","Collecting wrapt (from smart-open>=1.8.1->gensim)\n","  Downloading wrapt-1.17.2-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.4 kB)\n","Downloading gensim-4.3.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (26.7 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m26.7/26.7 MB\u001b[0m \u001b[31m59.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m85.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading scipy-1.13.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (38.6 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m38.6/38.6 MB\u001b[0m \u001b[31m14.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading smart_open-7.3.0.post1-py3-none-any.whl (61 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.9/61.9 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading wrapt-1.17.2-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (83 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m83.2/83.2 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: wrapt, numpy, smart-open, scipy, gensim\n","  Attempting uninstall: wrapt\n","    Found existing installation: wrapt 1.17.2\n","    Uninstalling wrapt-1.17.2:\n","      Successfully uninstalled wrapt-1.17.2\n","  Attempting uninstall: numpy\n","    Found existing installation: numpy 2.0.2\n","    Uninstalling numpy-2.0.2:\n","      Successfully uninstalled numpy-2.0.2\n","  Attempting uninstall: smart-open\n","    Found existing installation: smart_open 7.3.0.post1\n","    Uninstalling smart_open-7.3.0.post1:\n","      Successfully uninstalled smart_open-7.3.0.post1\n","  Attempting uninstall: scipy\n","    Found existing installation: scipy 1.15.3\n","    Uninstalling scipy-1.15.3:\n","      Successfully uninstalled scipy-1.15.3\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","opencv-python-headless 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n","thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\n","tsfresh 0.21.0 requires scipy>=1.14.0; python_version >= \"3.10\", but you have scipy 1.13.1 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed gensim-4.3.3 numpy-1.26.4 scipy-1.13.1 smart-open-7.3.0.post1 wrapt-1.17.2\n"]},{"output_type":"display_data","data":{"application/vnd.colab-display-data+json":{"pip_warning":{"packages":["numpy"]},"id":"9d0a07db490c4fc38e598fc046de0e97"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (1.5.1)\n","Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.26.4)\n","Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.13.1)\n","Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n"]}],"source":["# Restart runtime before running this cell for a fresh start\n","\n","#!pip install gensim --force-reinstall\n","#!pip install scikit-learn joblib\n"]},{"cell_type":"code","source":["!pip freeze > requirements.txt"],"metadata":{"id":"MCgBPlw-Wwqo","executionInfo":{"status":"ok","timestamp":1752889507413,"user_tz":240,"elapsed":3740,"user":{"displayName":"Yue Shang","userId":"00268558011417117003"}}},"execution_count":1,"outputs":[]},{"cell_type":"code","source":["from google.colab import files\n","files.download(\"requirements.txt\")"],"metadata":{"id":"rX8gmXFOW0nz","executionInfo":{"status":"ok","timestamp":1752889520999,"user_tz":240,"elapsed":52,"user":{"displayName":"Yue Shang","userId":"00268558011417117003"}},"outputId":"a4392056-a644-4e3d-b8e1-c1431f02dec7","colab":{"base_uri":"https://localhost:8080/","height":17}},"execution_count":2,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["\n","    async function download(id, filename, size) {\n","      if (!google.colab.kernel.accessAllowed) {\n","        return;\n","      }\n","      const div = document.createElement('div');\n","      const label = document.createElement('label');\n","      label.textContent = `Downloading \"${filename}\": `;\n","      div.appendChild(label);\n","      const progress = document.createElement('progress');\n","      progress.max = size;\n","      div.appendChild(progress);\n","      document.body.appendChild(div);\n","\n","      const buffers = [];\n","      let downloaded = 0;\n","\n","      const channel = await google.colab.kernel.comms.open(id);\n","      // Send a message to notify the kernel that we're ready.\n","      channel.send({})\n","\n","      for await (const message of channel.messages) {\n","        // Send a message to notify the kernel that we're ready.\n","        channel.send({})\n","        if (message.buffers) {\n","          for (const buffer of message.buffers) {\n","            buffers.push(buffer);\n","            downloaded += buffer.byteLength;\n","            progress.value = downloaded;\n","          }\n","        }\n","      }\n","      const blob = new Blob(buffers, {type: 'application/binary'});\n","      const a = document.createElement('a');\n","      a.href = window.URL.createObjectURL(blob);\n","      a.download = filename;\n","      div.appendChild(a);\n","      a.click();\n","      div.remove();\n","    }\n","  "]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["download(\"download_71342790-f450-4240-9e81-33191628ad46\", \"requirements.txt\", 13076)"]},"metadata":{}}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"__vFfhGruqHS"},"outputs":[],"source":["#!pip install pyLDAvis\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"foJVu-g_oyyF"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n","from sklearn.decomposition import LatentDirichletAllocation\n","from gensim.models import Word2Vec\n","from gensim.utils import simple_preprocess\n","import joblib\n","from sklearn.metrics.pairwise import cosine_similarity\n","\n","\n","# Load the Excel file\n","df = pd.read_excel('/content/drive/My Drive/Colab/AS4/STEP1-data_prepare/final_labeled_dataset.xlsx')\n","\n","# Extract the list of documents (texts) you want to use for LDA\n","my_texts = df['Text'].astype(str).tolist()  # make sure all texts are strings\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qTfSVLrFp8QO"},"outputs":[],"source":["import spacy\n","import en_core_web_sm\n","\n","nlp = en_core_web_sm.load()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xlsQVHD2qNS7"},"outputs":[],"source":["# === Original functions ===\n","def bag_of_words(texts):\n","    vectorizer = CountVectorizer()\n","    X = vectorizer.fit_transform(texts)\n","    return X.toarray(), vectorizer\n","\n","def n_grams(texts, n=2):\n","    vectorizer = CountVectorizer(ngram_range=(n, n))\n","    X = vectorizer.fit_transform(texts)\n","    return X.toarray(), vectorizer\n","\n","def tfidf(texts):\n","    vectorizer = TfidfVectorizer()\n","    X = vectorizer.fit_transform(texts)\n","    return X.toarray(), vectorizer\n","\n","def extract_ner_features(texts):\n","    features = []\n","    for text in texts:\n","        doc = nlp(text)\n","        ents = [ent.label_ for ent in doc.ents]\n","        ent_counts = {label: ents.count(label) for label in set(ents)}\n","        features.append(ent_counts)\n","    return features\n","\n","def word2vec_encoding(texts, vector_size=100, window=5, min_count=1):\n","    tokenized_texts = [simple_preprocess(doc) for doc in texts]\n","    model = Word2Vec(sentences=tokenized_texts, vector_size=vector_size, window=window,\n","                     min_count=min_count, workers=4, seed=42)\n","    doc_vectors = []\n","    for tokens in tokenized_texts:\n","        vectors = [model.wv[word] for word in tokens if word in model.wv]\n","        if vectors:\n","            doc_vectors.append(np.mean(vectors, axis=0))\n","        else:\n","            doc_vectors.append(np.zeros(vector_size))\n","    return np.array(doc_vectors), model\n","\n","from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n","from gensim.utils import simple_preprocess\n","import numpy as np\n","\n","def doc2vec_encoding(texts, vector_size=100, window=5, min_count=1, epochs=40):\n","    # Tag each document with an ID for training\n","    tagged_docs = [TaggedDocument(words=simple_preprocess(doc), tags=[str(i)]) for i, doc in enumerate(texts)]\n","\n","    # Initialize and train Doc2Vec model\n","    model = Doc2Vec(vector_size=vector_size, window=window, min_count=min_count, workers=4, seed=42, epochs=epochs)\n","    model.build_vocab(tagged_docs)\n","    model.train(tagged_docs, total_examples=model.corpus_count, epochs=model.epochs)\n","\n","    # Get vectors for each document by tag\n","    doc_vectors = np.array([model.dv[str(i)] for i in range(len(texts))])\n","\n","    return doc_vectors, model\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vUJPeat4rIQJ"},"outputs":[],"source":["from gensim.corpora.dictionary import Dictionary\n","from gensim.models.ldamodel import LdaModel\n","from gensim.models.ldamulticore import LdaMulticore\n","from gensim.models import CoherenceModel\n","\n","def prepare_corpus(texts):\n","    tokenized_texts = [text.split() for text in texts]  # or use more sophisticated tokenizer\n","    dictionary = Dictionary(tokenized_texts)\n","    corpus = [dictionary.doc2bow(text) for text in tokenized_texts]\n","    return tokenized_texts, dictionary, corpus\n","\n","def evaluate_lda_coherence_gensim(texts, topic_range=range(4, 7)):\n","    tokenized_texts, dictionary, corpus = prepare_corpus(texts)\n","    scores = []\n","    for n in topic_range:\n","        model = LdaMulticore(\n","            corpus=corpus,\n","            id2word=dictionary,\n","            num_topics=n,\n","            random_state=42,\n","            passes=5,\n","            iterations=50,\n","            workers=4  # Adjust this to your CPU cores\n","        )\n","        cm = CoherenceModel(model=model, texts=tokenized_texts, dictionary=dictionary, coherence='c_v')\n","        scores.append((n, cm.get_coherence()))\n","    return scores\n","\n","def lda_features_gensim(texts, n_topics, passes=20, iterations=100):\n","    tokenized_texts, dictionary, corpus = prepare_corpus(texts)\n","    lda_model = LdaMulticore(\n","        corpus=corpus,\n","        id2word=dictionary,\n","        num_topics=n_topics,\n","        random_state=42,\n","        passes=passes,\n","        iterations=iterations,\n","        workers=4\n","    )\n","    features = []\n","    for bow in corpus:\n","        doc_topics = lda_model.get_document_topics(bow, minimum_probability=0.0)\n","        features.append([prob for _, prob in doc_topics])\n","    return features, lda_model, dictionary\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iGa41raZwbcV"},"outputs":[],"source":["lda_vectors, lda_model, dictionary = lda_features_gensim(my_texts, n_topics=5)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fEheTtByxxhK"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","import joblib\n","from gensim.models import Word2Vec\n","from gensim.models.ldamulticore import LdaMulticore\n","from gensim.corpora import Dictionary\n","\n","# === Load dataset ===\n","df = pd.read_excel('/content/drive/My Drive/Colab/AS4/STEP1-data_prepare/final_labeled_dataset.xlsx')\n","my_texts = df['Text'].astype(str).tolist()\n","labels = df['Label']\n","numeric_labels = pd.factorize(labels)[0]\n","# print(numeric_labels)\n","\n","# === Apply and Save Feature Sets ===\n","def apply_and_save_features():\n","    output_dir = '/content/drive/My Drive/Colab/AS4/STEP2-feature_Engineering'\n","    os.makedirs(output_dir, exist_ok=True)\n","\n","    # 1. Bag of Words\n","    bow_matrix, bow_vectorizer = bag_of_words(my_texts)\n","    df_bow = pd.DataFrame(bow_matrix, columns=[f'bow_{i}' for i in range(bow_matrix.shape[1])])\n","    if labels is not None:\n","        df_bow['label'] = labels\n","        df_bow['label_num'] = numeric_labels\n","    df_bow.to_pickle(f'{output_dir}/features_bow.pkl')\n","    joblib.dump(bow_vectorizer, f'{output_dir}/vectorizer_bow.pkl')\n","\n","    # 2. TF-IDF\n","    tfidf_matrix, tfidf_vectorizer = tfidf(my_texts)\n","    df_tfidf = pd.DataFrame(tfidf_matrix, columns=[f'tfidf_{i}' for i in range(tfidf_matrix.shape[1])])\n","    if labels is not None:\n","        df_tfidf['label'] = labels\n","        df_tfidf['label_num'] = numeric_labels\n","    df_tfidf.to_pickle(f'{output_dir}/features_tfidf.pkl')\n","    joblib.dump(tfidf_vectorizer, f'{output_dir}/vectorizer_tfidf.pkl')\n","\n","    # 3. N-Grams (bi-gram)\n","    ngram_matrix, ngram_vectorizer = n_grams(my_texts, n=2)\n","    df_ngram = pd.DataFrame(ngram_matrix, columns=[f'ngram2_{i}' for i in range(ngram_matrix.shape[1])])\n","    if labels is not None:\n","        df_ngram['label'] = labels\n","        df_ngram['label_num'] = numeric_labels\n","    df_ngram.to_pickle(f'{output_dir}/features_ngram2.pkl')\n","    joblib.dump(ngram_vectorizer, f'{output_dir}/vectorizer_ngram2.pkl')\n","\n","    # 4. LDA\n","    lda_vectors, lda_model, dictionary = lda_features_gensim(my_texts, n_topics=5, passes=20, iterations=100)\n","    df_lda = pd.DataFrame(lda_vectors, columns=[f'lda_{i}' for i in range(len(lda_vectors[0]))])\n","    if labels is not None:\n","        df_lda['label'] = labels\n","        df_lda['label_num'] = numeric_labels\n","    df_lda.to_pickle(f'{output_dir}/features_lda.pkl')\n","    lda_model.save(f'{output_dir}/lda_model_5topics.model')\n","    dictionary.save(f'{output_dir}/lda_dictionary.dict')\n","\n","    # 5. Word2Vec\n","    w2v_matrix, w2v_model = word2vec_encoding(my_texts)\n","    df_w2v = pd.DataFrame(w2v_matrix, columns=[f'w2v_{i}' for i in range(w2v_matrix.shape[1])])\n","    if labels is not None:\n","        df_w2v['label'] = labels\n","        df_w2v['label_num'] = numeric_labels\n","    df_w2v.to_pickle(f'{output_dir}/features_word2vec.pkl')\n","    w2v_model.save(f'{output_dir}/word2vec.model')\n","\n","    # 6. Doc2Vec\n","    d2v_matrix, d2v_model = doc2vec_encoding(my_texts)\n","    df_d2v = pd.DataFrame(d2v_matrix, columns=[f'd2v_{i}' for i in range(d2v_matrix.shape[1])])\n","    if labels is not None:\n","        df_d2v['label'] = labels\n","        df_d2v['label_num'] = numeric_labels\n","    df_d2v.to_pickle(f'{output_dir}/features_doc2vec.pkl')\n","    d2v_model.save(f'{output_dir}/doc2vec.model')\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vdF1wevQRmN4"},"outputs":[],"source":["pd.DataFrame({\n","    'text': df['Text'].astype(str).tolist(),\n","    'label': df['Label'],\n","    'label_num': pd.factorize(labels)[0]\n","}).to_pickle('/content/drive/My Drive/Colab/AS4/STEP2-feature_Engineering/raw_labeled.pkl')\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MQ89V0H9RmN4"},"outputs":[],"source":["import os\n","\n","apply_and_save_features()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VHx3xi_g2jL9","executionInfo":{"status":"ok","timestamp":1752849860244,"user_tz":240,"elapsed":57,"user":{"displayName":"Yue Shang","userId":"00268558011417117003"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"74b901e9-89c9-4283-a437-632c78ad81c7"},"outputs":[{"output_type":"stream","name":"stdout","text":["features_bow.pkl:  Found\n","vectorizer_bow.pkl:  Found\n","features_tfidf.pkl:  Found\n","vectorizer_tfidf.pkl:  Found\n","features_ngram2.pkl:  Found\n","vectorizer_ngram2.pkl:  Found\n","features_lda.pkl:  Found\n","lda_model_5topics.model:  Found\n","lda_dictionary.dict:  Found\n","features_word2vec.pkl:  Found\n","word2vec.model:  Found\n","features_doc2vec.pkl:  Found\n","doc2vec.model:  Found\n"]}],"source":["import os\n","\n","output_dir = '/content/drive/My Drive/Colab/AS4/STEP2-feature_Engineering'\n","files = [\n","    'features_bow.pkl', 'vectorizer_bow.pkl',\n","    'features_tfidf.pkl', 'vectorizer_tfidf.pkl',\n","    'features_ngram2.pkl', 'vectorizer_ngram2.pkl',\n","    'features_lda.pkl', 'lda_model_5topics.model', 'lda_dictionary.dict',\n","    'features_word2vec.pkl', 'word2vec.model',\n","    'features_doc2vec.pkl', 'doc2vec.model'\n","]\n","\n","for f in files:\n","    full_path = os.path.join(output_dir, f)\n","    print(f\"{f}: {' Found' if os.path.exists(full_path) else ' Missing'}\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ExXJm0zDRmN6","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1752867905089,"user_tz":240,"elapsed":3044,"user":{"displayName":"Yue Shang","userId":"00268558011417117003"}},"outputId":"ae883673-5157-491c-9aa6-16a1902967c6"},"outputs":[{"output_type":"stream","name":"stdout","text":["TF-IDF matrix created and saved successfully.\n"]}],"source":["import pandas as pd\n","from scipy.sparse import save_npz\n","import joblib\n","\n","# Load dataset with 'Text' column (your cleaned and preprocessed text)\n","df = pd.read_excel('/content/drive/My Drive/Colab/AS4/STEP1-data_prepare/final_labeled_dataset.xlsx')\n","df['Text'] = df['Text'].fillna('')\n","\n","# Load existing vectorizer\n","vectorizer = joblib.load('/content/drive/My Drive/Colab/AS4/STEP2-feature_Engineering/vectorizer_tfidf.pkl')\n","\n","# Transform text data (do NOT fit again!)\n","tfidf_matrix = vectorizer.transform(df['Text'])\n","\n","# Save the sparse matrix\n","save_npz('/content/drive/My Drive/Colab/AS4/STEP2-feature_Engineering/tfidf_matrix.npz', tfidf_matrix)\n","\n","print(\"TF-IDF matrix created and saved successfully.\")\n"]}],"metadata":{"colab":{"provenance":[{"file_id":"https://github.com/EigentlicherVogel/dti-5125-p1-con/blob/cc79b8d07cb1167f897ec2909423b5db74de81f0/STEP2_Feature_Engineering/STEP2_Feature_Engineering_Colab.ipynb","timestamp":1752721853976}]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.3"}},"nbformat":4,"nbformat_minor":0}